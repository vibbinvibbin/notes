#!/bin/bash

# IBRAHIMXSS Methodology for Crawling Potential URLs for XSS Testing

# Crawling dead & alive URLs
waybackurls http://testphp.vulnweb.com | tee -a 1.txt
getallurls http://testphp.vulnweb.com  | tee -a 2.txt
gau http://testphp.vulnweb.com | tee -a 3.txt
python waymore.py -i http://testphp.vulnweb.com -mode U -oU 4.txt
katana -u http://testphp.vulnweb.com -kf 3 | tee -a 5.txt

# Merging files into one
cat *.txt > output.txt

# Sorting unique URLs
cat output.txt | uniq > unique_urls1.txt

# Enumerating domains
subdominator -d testphp.vulnweb.com -o domains.txt

# Crawling links
cat domains.txt | hakrawler | tee -a links.txt
subprober -f domains.txt -sc -ar -o 200-codes-urls.txt -nc -mc 200 -c 30

# Extension filter
cat urls.txt | grep -E -v '\.css$|\.js$|\.jpg$|\.JPG$|\.PNG$|\.GIF$|\.avi$|\.dll$|\.pl$|\.webm$|\.c$|\.py$|\.bat$|\.tar$|\.swp$|\.tmp$|\.sh$|\.deb$|\.exe$|\.zip$|\.mpeg$|\.mpg$|\.flv$|\.wmv$|\.wma$|\.aac$|\.m4a$|\.ogg$|\.mp4$|\.mp3$|\.bat$|\.dat$|\.cfg$|\.cfm$|\.bin$|\.jpeg$|\.JPEG$|\.ps.gz$|\.gz$|\.gif$|\.tif$|\.tiff$|\.csv$|\.png$|\.ttf$|\.ppt$|\.pptx$|\.ppsx$|\.doc$|\.woff$|\.xlsx$|\.xls$|\.mpp$|\.mdb$|\.json$|\.woff2$|\.icon$|\.pdf$|\.docx$|\.svg$|\.txt$|\.jar$|\.0$|\.1$|\.2$|\.3$|\.4$|\.m4r$|\.kml$|\.pro$|\.yao$|\.gcn3$|\.PDF$|\.egy$|\.par$|\.lin$|\.yht$' > filtered_links.txt

# Removing duplicates using uro
cat filtered_links.txt | uro -b css js jpg JPG PNG GIF avi dll pl webm c py bat tar swp tmp sh deb exe zip mpeg mpg flv wmv wma aac m4a ogg mp4 mp3 bat dat cfg cfm bin jpeg JPEG ps.gz gz gif tif tiff csv png ttf ppt pptx ppsx doc woff xlsx xls mpp mdb json woff2 icon pdf docx svg txt jar 0 1 2 3 4 m4r kml pro yao gcn3 PDF egy par lin yht | tee -a lista.txt

cat lista.txt | uniq > unique_urls2.txt

# Filtering
grep '=' unique_urls2.txt > filtered_urls_with_params.txt  # For query URLs
grep -v '=' unique_urls2.txt > filtered_urls_without_params.txt  # For path URLs

# Filtering specific file extensions for XSS & SQLi tools
cat unique_urls2.txt | grep -E "\.php|\.asp|\.aspx|\.cfm|\.jsp" | sort > output.txt

# For URLs without any extension
grep -v "http[^ ]*\.[^/]*\." unique_urls2.txt | grep "http" | sort > clean_urls.txt

